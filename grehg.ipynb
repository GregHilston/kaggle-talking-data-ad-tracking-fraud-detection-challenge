{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TalkingData AdTracking Fraud Detection Challenge](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/kernels?sortBy=hotness&group=everyone&pageSize=20&language=Python&competitionId=8540)\n",
    "\n",
    "Can you detect fraudulent click traffic for mobile app ads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This notebook has crashed every machine I ran it on with less than 60.0 GBs of RAM.\n",
    "\n",
    "_I'm sorry_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure our graphs are displayed inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make wider graphs\n",
    "sns.set(rc={'figure.figsize':(12,5)});\n",
    "plt.figure(figsize=(12,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first look at the data files we've downloaded for the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sizes\n",
      "train_sample.csv              4.08 MB\n",
      "train.csv                     7537.65 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"File sizes\")\n",
    "\n",
    "for f in os.listdir(\"data/\"):\n",
    "    if \"zip\" not in f and f.endswith(\"csv\"):\n",
    "        print(f.ljust(30) + str(round(os.path.getsize(\"data/\" + f) / 1000000, 2)) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we'd like to work with a small data set, like `train_sample.csv`, its just too small to represent the entire data set `train.csv`. We'll load the entire train set into a DataFrame so we can analyze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_sample.csv\") # , dtype=dtype, parse_dates=parse_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets peak at the first few values of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll notice that the features `ip`, `app`, `device`, `os` and `channel` and our class variable `is_attributed` are categorical as they're encoded to anonymize and preserve privacy. Therefore we'll want to ensure we set their type to non-numerical to avoid nonense operations on the data like calculating their `mean`, `median`, ... etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"ip\", \"app\", \"device\", \"os\", \"channel\", \"is_attributed\"]\n",
    "\n",
    "for column in categorical_columns:\n",
    "    train[column] = train[column].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we'll covert the `click_time` and `attributed_time` columns into date time fields, as they represent time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['click_time'] = pd.to_datetime(train['click_time'])\n",
    "train['attributed_time'] = pd.to_datetime(train['attributed_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get a high level look at the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "cols = [\"ip\", \"app\", \"device\", \"os\", \"channel\"]\n",
    "uniques = [len(train[col].unique()) for col in cols]\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.barplot(cols, uniques, log=True)\n",
    "ax.set(xlabel=\"Feature\", ylabel=\"log(unique count)\", title=\"Number of unique values per feature\")\n",
    "\n",
    "# Places the value just above the column\n",
    "for p, uniq in zip(ax.patches, uniques):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 20,\n",
    "            uniq,\n",
    "            ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our knowledge of the competion, every row in the DataFrame that has a set value of `is_attributed` should also have a value for `attributed_time`. Lets test that belief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs a subset of the DataFrame and then further grabs only the rows where `is_attributed` is set, then calculating the counts\n",
    "train[['attributed_time', 'is_attributed']][train['is_attributed']==1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Take Aways\n",
    "\n",
    "- The training set takes place over two days, two hours and eleven seconds\n",
    "- Out of 184,903,890 rows, only 456,846 of them have an `attributed_time` values of `1.0`\n",
    "  - This means only 456,846 out of 184,903,890 ad clicks resulted in a download\n",
    "  - Which is about 0.0025 % of the clicks\n",
    "- There is atleast one ip adress that triggers an ad click over fifty thousand times\n",
    "  - Seems strange that one ip address would click that often in a span of just 4 days\n",
    "  - Does that mean that ip address encoded is not device id, but network id? (explore this below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is incredilby unbalanced. We're visualizing here the small percents of ad clicks resulting in a download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "#sns.set(font_scale=1.2)\n",
    "mean = (train.is_attributed.values == 1).mean()\n",
    "ax = sns.barplot(['App Downloaded (1)', 'Not Downloaded (0)'], [mean, 1-mean])\n",
    "ax.set(ylabel='Proportion', title='App Downloaded vs Not Downloaded')\n",
    "\n",
    "for p, uniq in zip(ax.patches, [mean, 1-mean]):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height+0.01,\n",
    "            '{}%'.format(round(uniq * 100, 2)),\n",
    "            ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore ip counts: _Check if multuiple ips have any downloads_\n",
    "\n",
    "Since we don't know what `ip` is actually encoding, we're going to see if we can make any inferences based on the `value_counts()` of tha data set.\n",
    "\n",
    "One might think that each `ip` equates to a single user, but we'll see that this is probably not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary table to see ips with their associated count frequencies\n",
    "temp = train['ip'].value_counts().reset_index(name='counts')\n",
    "temp.columns = ['ip', 'counts']\n",
    "temp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add temporary counts of ip feature ('counts') to the train table, to see if IPs with high counts have conversions\n",
    "train= train.merge(temp, on='ip', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check top 10 values\n",
    "train[train['is_attributed']==1].sort_values('counts', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['is_attributed']==1].ip.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see up to `2340` downloads for a single IP address. This IP must be for some sort of network with multiple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'is_attributed' back to numeric for proportion calculations\n",
    "train['is_attributed']=train['is_attributed'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion rates over Counts of 300 most popular IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['ip', 'is_attributed']].groupby('ip', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['ip', 'is_attributed']].groupby('ip', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='ip', how='left')\n",
    "merge.columns = ['ip', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:300].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 300 Most Popular IPs')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular IPs')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be a correlation between the popularity of an `ip` and its `click_count`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by App\n",
    "\n",
    "We'll check out the 100 most popular apps by click count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['app', 'is_attributed']].groupby('app', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['app', 'is_attributed']].groupby('app', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='app', how='left')\n",
    "merge.columns = ['app', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:100].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 100 Most Popular Apps')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular Apps')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here a very large difference in the `click_count` per `app`. The largest `click_count` is thirty three million for one app.\n",
    "\n",
    "We can explain the proportion flucuation as the `click_count` reduces as each click will have a larger impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by OS\n",
    "\n",
    "Now we'll look at the top 100 operating systems by `click_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['os', 'is_attributed']].groupby('os', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['os', 'is_attributed']].groupby('os', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='os', how='left')\n",
    "merge.columns = ['os', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:100].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 100 Most Popular Operating Systems')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular Operating Systems')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agin, we can see ratio is very low but as the `click_count` reduces the ratio starts fluxuating more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['device', 'is_attributed']].groupby('device', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['device', 'is_attributed']].groupby('device', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='device', how='left')\n",
    "merge.columns = ['device', 'click_count', 'prop_downloaded']\n",
    "\n",
    "print('Count of clicks and proportion of downloads by device:')\n",
    "print(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['channel', 'is_attributed']].groupby('channel', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['channel', 'is_attributed']].groupby('channel', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='channel', how='left')\n",
    "merge.columns = ['channel', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:100].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 100 Most Popular Apps')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular Channels')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some random peaks for some `channel`s at high `click_count`s, but generally we're seeing the same situation as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Time Patterns\n",
    "\n",
    "We're now going to inspect hourly patterns by rounding the `click_time` down to an hour of the samme day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert click_time and attributed_time to time series\n",
    "train['click_time'] = pd.to_datetime(train['click_time'])\n",
    "train['attributed_time'] = pd.to_datetime(train['attributed_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round the time to nearest hour\n",
    "train['click_rnd']=train['click_time'].dt.round('H')  \n",
    "\n",
    "#check for hourly patterns\n",
    "train[['click_rnd','is_attributed']].groupby(['click_rnd'], as_index=True).count().plot()\n",
    "plt.title('HOURLY CLICK FREQUENCY');\n",
    "plt.ylabel('Number of Clicks');\n",
    "\n",
    "train[['click_rnd','is_attributed']].groupby(['click_rnd'], as_index=True).mean().plot()\n",
    "plt.title('HOURLY CONVERSION RATIO');\n",
    "plt.ylabel('Converted Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's clearly a pattern in frequency of clicks based on time of day, but there is no clear hourly time pattern in ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a new feature by extracting the hour of the day, for every day. We'll be checking if the combined if there's a trend specifically based on the hour throughout the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract hour as a feature\n",
    "train['click_hour']=train['click_time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll inspect the clicks by hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).count().plot(kind='bar', color='#a675a1')\n",
    "plt.title('HOURLY CLICK FREQUENCY Barplot');\n",
    "plt.ylabel('Number of Clicks');\n",
    "\n",
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).count().plot(color='#a675a1')\n",
    "plt.title('HOURLY CLICK FREQUENCY Lineplot');\n",
    "plt.ylabel('Number of Clicks');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, number of conversions by hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).mean().plot(kind='bar', color='#75a1a6')\n",
    "plt.title('HOURLY CONVERSION RATIO Barplot');\n",
    "plt.ylabel('Converted Ratio');\n",
    "\n",
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).mean().plot( color='#75a1a6')\n",
    "plt.title('HOURLY CONVERSION RATIO Lineplot');\n",
    "plt.ylabel('Converted Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its hard to compare the clicks and conversions per hour, so lets overlay the two Lineplots to see if there's any easy to see correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://stackoverflow.com/questions/9103166/multiple-axis-in-matplotlib-with-different-scales\n",
    "#smonek's answer\n",
    "\n",
    "\n",
    "group = train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=False).mean()\n",
    "x = group['click_hour']\n",
    "ymean = group['is_attributed']\n",
    "group = train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=False).count()\n",
    "ycount = group['is_attributed']\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "host = fig.add_subplot(111)\n",
    "\n",
    "par1 = host.twinx()\n",
    "\n",
    "host.set_xlabel(\"Time\")\n",
    "host.set_ylabel(\"Proportion Converted\")\n",
    "par1.set_ylabel(\"Click Count\")\n",
    "\n",
    "#color1 = plt.cm.viridis(0)\n",
    "#color2 = plt.cm.viridis(0.5)\n",
    "color1 = '#75a1a6'\n",
    "color2 = '#a675a1'\n",
    "\n",
    "p1, = host.plot(x, ymean, color=color1,label=\"Proportion Converted\")\n",
    "p2, = par1.plot(x, ycount, color=color2, label=\"Click Count\")\n",
    "\n",
    "lns = [p1, p2]\n",
    "host.legend(handles=lns, loc='best')\n",
    "\n",
    "host.yaxis.label.set_color(p1.get_color())\n",
    "par1.yaxis.label.set_color(p2.get_color())\n",
    "\n",
    "plt.savefig(\"pyplot_multiple_y-axis.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only take away I have from this graph is the significant down trend in both Proportion Converted and Click Count that starts around the twelth hour.\n",
    "\n",
    "I am currently unsure what time this would actually represent, nor if it even matters.\n",
    "\n",
    "We'll produce one more graphic to see the `click_hour` vs Converted Ratio, with an error bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('click_hour', 'is_attributed', data=train)\n",
    "plt.title('HOURLY CONVERSION RATIO');\n",
    "plt.ylabel('Converted Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look intro `attributed_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look specifically at conversions that did take place. We'll explicitly be looking at how much time passed from the original ad click to the actual download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['timePass']= train['attributed_time']-train['click_time']\n",
    "#check:\n",
    "train[train['is_attributed']==1][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['timePass'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the longest time it took to go from clicking an ad to downloading the app was about twenty four hours, while the shortest time was zero seconds.\n",
    "\n",
    "Both this minimum value and maximum value seem incredibly fishy.\n",
    "\n",
    "I was reading through the comments on [yuliagm's post](https://www.kaggle.com/yuliagm/talkingdata-eda-plus-time-patterns) and saw the community discussing what exacty the `attributed_time` and `click_time` might represent. User [shlomis](https://www.kaggle.com/shlomis) suggests that the size of the app would affect the installation time, as the timestamp wouldn't get triggered till the end of the download. So for example, a 100MB app would take longer than say a 10MB app, but [Yuliagm](https://www.kaggle.com/yuliagm) shot that idea down.\n",
    "\n",
    "Additionally, [Peter](https://www.kaggle.com/pestipeti) suggested that the download/install tracking code is in the application itself, atleast for Android, and therefore the `attributed_time` will not get triggered until the user opens the application for the first time. If this was true, this would explain the rather large `timePass` columns we calculated, as someone could wait almost twenty four hours before opening their app. I find this fishy, as I'd expect the max to be even larger than this. \n",
    "\n",
    "At this point, I nor does anyone I've seen on Kaggle have a definitive idea what these columns represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `click_time` Pandas.DateTime Transformation\n",
    "\n",
    "To make training our model(s) easier, we'll translate our two `datetime64` columns, `click_time` and `attributed_time`, to separate columns that can be represented with `int`s.\n",
    "\n",
    "Specifically:\n",
    "* year\n",
    "* month\n",
    "* day\n",
    "* hour\n",
    "* minute\n",
    "* second\n",
    "\n",
    "We'll be doing this process multicored, as it takes about ten minutes to run on my EC2 `r3.2xlarge`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train[\"year\"], train[\"month\"], train[\"day\"], train[\"hour\"], train[\"minute\"], train[\"second\"] = zip(*train[\"click_time\"].apply(lambda row: (row.year, row.month, row.day, row.hour, row.minute, row.second)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses all processessors\n",
    "executor = ProcessPoolExecutor(max_workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(row):\n",
    "    \"\"\"Converts a single row from Pandas DateTime to individual elements\n",
    "    \"\"\"\n",
    "    \n",
    "    return (row.year, row.month, row.day, row.hour, row.minute, row.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# kicking off all calculations\n",
    "for _, row in train.iterrows():\n",
    "    results.append(executor.submit(convert_date, row[\"click_time\"]))\n",
    "\n",
    "# blocking for result\n",
    "results = [i.result() for i in results]\n",
    "\n",
    "train[\"year\"], train[\"month\"], train[\"day\"], train[\"hour\"], train[\"minute\"], train[\"second\"] = zip(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the first couple of rows to ensure we separated the values correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87540</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>497</td>\n",
       "      <td>2017-11-07 09:30:38</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105560</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>259</td>\n",
       "      <td>2017-11-07 13:40:27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101424</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>212</td>\n",
       "      <td>2017-11-07 18:05:24</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94584</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>477</td>\n",
       "      <td>2017-11-07 04:58:08</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68413</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>2017-11-09 09:00:09</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel          click_time attributed_time  \\\n",
       "0   87540  12      1  13     497 2017-11-07 09:30:38             NaT   \n",
       "1  105560  25      1  17     259 2017-11-07 13:40:27             NaT   \n",
       "2  101424  12      1  19     212 2017-11-07 18:05:24             NaT   \n",
       "3   94584  13      1  13     477 2017-11-07 04:58:08             NaT   \n",
       "4   68413  12      1   1     178 2017-11-09 09:00:09             NaT   \n",
       "\n",
       "  is_attributed  year  month  day  hour  minute  second  \n",
       "0             0  2017     11    7     9      30      38  \n",
       "1             0  2017     11    7    13      40      27  \n",
       "2             0  2017     11    7    18       5      24  \n",
       "3             0  2017     11    7     4      58       8  \n",
       "4             0  2017     11    9     9       0       9  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things look good! \n",
    "\n",
    "Now we'll drop the old column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['click_time'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1f4b88511040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"click_time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   2560\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2563\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3744\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3745\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['click_time'] not contained in axis"
     ]
    }
   ],
   "source": [
    "train = train.drop([\"click_time\"], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Data Leakage\n",
    "\n",
    "To avoid data leakage, our models will not be using the `attributed_time` columns, as this is directly correlated to the `is_attributed` and would result in a score of 100%. Additionaly, the `attributed_time` does not exist in the testing set.\n",
    "\n",
    "Lets remove `attributed_time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87540</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>497</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105560</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>259</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101424</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94584</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>477</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68413</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel is_attributed  year  month  day  hour  \\\n",
       "0   87540  12      1  13     497             0  2017     11    7     9   \n",
       "1  105560  25      1  17     259             0  2017     11    7    13   \n",
       "2  101424  12      1  19     212             0  2017     11    7    18   \n",
       "3   94584  13      1  13     477             0  2017     11    7     4   \n",
       "4   68413  12      1   1     178             0  2017     11    9     9   \n",
       "\n",
       "   minute  second  \n",
       "0      30      38  \n",
       "1      40      27  \n",
       "2       5      24  \n",
       "3      58       8  \n",
       "4       0       9  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop([\"attributed_time\"], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Are first model we'll use is strictly logistic regression. Our plan here was to go from nothing to something as quickly as possible and than iterate and see if we can increase our ROC curve value.\n",
    "\n",
    "First we'll get a list of all our features, `x`, and our class variable, `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our x and y column names\n",
    "y_column = [\"is_attributed\"]\n",
    "x_columns = [i for i in train if i not in y_column]\n",
    "\n",
    "x = train[x_columns]\n",
    "y = train[y_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Logistic Regression\n",
    "\n",
    "Our first attempt will simply use every column as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7760330705989418"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_test, lr.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://www.kaggle.com/anokas/talkingdata-adtracking-eda\n",
    "  - for usage of inspecting files before choosing which to use for EDA\n",
    "* https://www.kaggle.com/yuliagm/talkingdata-eda-plus-time-patterns\n",
    "  - for excellent EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
