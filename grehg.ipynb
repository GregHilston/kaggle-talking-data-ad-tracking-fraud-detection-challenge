{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TalkingData AdTracking Fraud Detection Challenge](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/kernels?sortBy=hotness&group=everyone&pageSize=20&language=Python&competitionId=8540)\n",
    "\n",
    "Can you detect fraudulent click traffic for mobile app ads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This notebook has crashed every machine I ran it on with less than 120.0 GBs of RAM. I specifically was using a AWS EC2 x1e.xlarge instance. May use r4.4xlarge in the future, looks much faster for slightly more cost.\n",
    "\n",
    "_I'm sorry_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure our graphs are displayed inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make wider graphs\n",
    "sns.set(rc={'figure.figsize':(12,5)});\n",
    "plt.figure(figsize=(12,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first look at the data files we've downloaded for the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"File sizes\")\n",
    "\n",
    "for f in os.listdir(\"data/\"):\n",
    "    if \"zip\" not in f and f.endswith(\"csv\"):\n",
    "        print(f.ljust(30) + str(round(os.path.getsize(\"data/\" + f) / 1000000, 2)) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we'd like to work with a small data set, like `train_sample.csv`, its just too small to represent the entire data set `train.csv`. We'll load the entire train set into a DataFrame so we can analyze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set = \"data/train_sample.csv\"\n",
    "data_set = \"data/train.csv\"\n",
    "\n",
    "train = pd.read_csv(data_set) # , dtype=dtype, parse_dates=parse_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets peak at the first few values of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll notice that the features `ip`, `app`, `device`, `os` and `channel` and our class variable `is_attributed` are categorical as they're encoded to anonymize and preserve privacy. Therefore we'll want to ensure we set their type to non-numerical to avoid nonense operations on the data like calculating their `mean`, `median`, ... etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"ip\", \"app\", \"device\", \"os\", \"channel\", \"is_attributed\"]\n",
    "\n",
    "for column in categorical_columns:\n",
    "    train[column] = train[column].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we'll covert the `click_time` and `attributed_time` columns into date time fields, as they represent time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['click_time'] = pd.to_datetime(train['click_time'])\n",
    "train['attributed_time'] = pd.to_datetime(train['attributed_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get a high level look at the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "cols = [\"ip\", \"app\", \"device\", \"os\", \"channel\"]\n",
    "uniques = [len(train[col].unique()) for col in cols]\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.barplot(cols, uniques, log=True)\n",
    "ax.set(xlabel=\"Feature\", ylabel=\"log(unique count)\", title=\"Number of unique values per feature\")\n",
    "\n",
    "# Places the value just above the column\n",
    "for p, uniq in zip(ax.patches, uniques):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 20,\n",
    "            uniq,\n",
    "            ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our knowledge of the competion, every row in the DataFrame that has a set value of `is_attributed` should also have a value for `attributed_time`. Lets test that belief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs a subset of the DataFrame and then further grabs only the rows where `is_attributed` is set, then calculating the counts\n",
    "train[['attributed_time', 'is_attributed']][train['is_attributed']==1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Take Aways\n",
    "\n",
    "- The training set takes place over two days, two hours and eleven seconds\n",
    "- Out of 184,903,890 rows, only 456,846 of them have an `attributed_time` values of `1.0`\n",
    "  - This means only 456,846 out of 184,903,890 ad clicks resulted in a download\n",
    "  - Which is about 0.0025 % of the clicks\n",
    "- There is atleast one ip adress that triggers an ad click over fifty thousand times\n",
    "  - Seems strange that one ip address would click that often in a span of just 4 days\n",
    "  - Does that mean that ip address encoded is not device id, but network id? (explore this below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is incredilby unbalanced. We're visualizing here the small percents of ad clicks resulting in a download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "#sns.set(font_scale=1.2)\n",
    "mean = (train.is_attributed.values == 1).mean()\n",
    "ax = sns.barplot(['App Downloaded (1)', 'Not Downloaded (0)'], [mean, 1-mean])\n",
    "ax.set(ylabel='Proportion', title='App Downloaded vs Not Downloaded')\n",
    "\n",
    "for p, uniq in zip(ax.patches, [mean, 1-mean]):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height+0.01,\n",
    "            '{}%'.format(round(uniq * 100, 2)),\n",
    "            ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore ip counts: _Check if multuiple ips have any downloads_\n",
    "\n",
    "Since we don't know what `ip` is actually encoding, we're going to see if we can make any inferences based on the `value_counts()` of tha data set.\n",
    "\n",
    "One might think that each `ip` equates to a single user, but we'll see that this is probably not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary table to see ips with their associated count frequencies\n",
    "temp = train['ip'].value_counts().reset_index(name='counts')\n",
    "temp.columns = ['ip', 'counts']\n",
    "temp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add temporary counts of ip feature ('counts') to the train table, to see if IPs with high counts have conversions\n",
    "train= train.merge(temp, on='ip', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check top 10 values\n",
    "train[train['is_attributed']==1].sort_values('counts', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['is_attributed']==1].ip.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see up to `2340` downloads for a single IP address. This IP must be for some sort of network with multiple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'is_attributed' back to numeric for proportion calculations\n",
    "train['is_attributed']=train['is_attributed'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion rates over Counts of 300 most popular IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['ip', 'is_attributed']].groupby('ip', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['ip', 'is_attributed']].groupby('ip', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='ip', how='left')\n",
    "merge.columns = ['ip', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:300].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 300 Most Popular IPs')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular IPs')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be a correlation between the popularity of an `ip` and its `click_count`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by App\n",
    "\n",
    "We'll check out the 100 most popular apps by click count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['app', 'is_attributed']].groupby('app', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['app', 'is_attributed']].groupby('app', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='app', how='left')\n",
    "merge.columns = ['app', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:100].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 100 Most Popular Apps')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular Apps')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here a very large difference in the `click_count` per `app`. The largest `click_count` is thirty three million for one app.\n",
    "\n",
    "We can explain the proportion flucuation as the `click_count` reduces as each click will have a larger impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by OS\n",
    "\n",
    "Now we'll look at the top 100 operating systems by `click_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['os', 'is_attributed']].groupby('os', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['os', 'is_attributed']].groupby('os', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='os', how='left')\n",
    "merge.columns = ['os', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:100].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 100 Most Popular Operating Systems')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular Operating Systems')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agin, we can see ratio is very low but as the `click_count` reduces the ratio starts fluxuating more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['device', 'is_attributed']].groupby('device', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['device', 'is_attributed']].groupby('device', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='device', how='left')\n",
    "merge.columns = ['device', 'click_count', 'prop_downloaded']\n",
    "\n",
    "print('Count of clicks and proportion of downloads by device:')\n",
    "print(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['channel', 'is_attributed']].groupby('channel', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['channel', 'is_attributed']].groupby('channel', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='channel', how='left')\n",
    "merge.columns = ['channel', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:100].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 100 Most Popular Apps')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular Channels')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some random peaks for some `channel`s at high `click_count`s, but generally we're seeing the same situation as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Time Patterns\n",
    "\n",
    "We're now going to inspect hourly patterns by rounding the `click_time` down to an hour of the samme day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert click_time and attributed_time to time series\n",
    "train['click_time'] = pd.to_datetime(train['click_time'])\n",
    "train['attributed_time'] = pd.to_datetime(train['attributed_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round the time to nearest hour\n",
    "train['click_rnd']=train['click_time'].dt.round('H')  \n",
    "\n",
    "#check for hourly patterns\n",
    "train[['click_rnd','is_attributed']].groupby(['click_rnd'], as_index=True).count().plot()\n",
    "plt.title('HOURLY CLICK FREQUENCY');\n",
    "plt.ylabel('Number of Clicks');\n",
    "\n",
    "train[['click_rnd','is_attributed']].groupby(['click_rnd'], as_index=True).mean().plot()\n",
    "plt.title('HOURLY CONVERSION RATIO');\n",
    "plt.ylabel('Converted Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's clearly a pattern in frequency of clicks based on time of day, but there is no clear hourly time pattern in ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a new feature by extracting the hour of the day, for every day. We'll be checking if the combined if there's a trend specifically based on the hour throughout the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract hour as a feature\n",
    "train['click_hour']=train['click_time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll inspect the clicks by hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).count().plot(kind='bar', color='#a675a1')\n",
    "plt.title('HOURLY CLICK FREQUENCY Barplot');\n",
    "plt.ylabel('Number of Clicks');\n",
    "\n",
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).count().plot(color='#a675a1')\n",
    "plt.title('HOURLY CLICK FREQUENCY Lineplot');\n",
    "plt.ylabel('Number of Clicks');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, number of conversions by hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).mean().plot(kind='bar', color='#75a1a6')\n",
    "plt.title('HOURLY CONVERSION RATIO Barplot');\n",
    "plt.ylabel('Converted Ratio');\n",
    "\n",
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).mean().plot( color='#75a1a6')\n",
    "plt.title('HOURLY CONVERSION RATIO Lineplot');\n",
    "plt.ylabel('Converted Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its hard to compare the clicks and conversions per hour, so lets overlay the two Lineplots to see if there's any easy to see correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://stackoverflow.com/questions/9103166/multiple-axis-in-matplotlib-with-different-scales\n",
    "#smonek's answer\n",
    "\n",
    "\n",
    "group = train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=False).mean()\n",
    "x = group['click_hour']\n",
    "ymean = group['is_attributed']\n",
    "group = train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=False).count()\n",
    "ycount = group['is_attributed']\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "host = fig.add_subplot(111)\n",
    "\n",
    "par1 = host.twinx()\n",
    "\n",
    "host.set_xlabel(\"Time\")\n",
    "host.set_ylabel(\"Proportion Converted\")\n",
    "par1.set_ylabel(\"Click Count\")\n",
    "\n",
    "#color1 = plt.cm.viridis(0)\n",
    "#color2 = plt.cm.viridis(0.5)\n",
    "color1 = '#75a1a6'\n",
    "color2 = '#a675a1'\n",
    "\n",
    "p1, = host.plot(x, ymean, color=color1,label=\"Proportion Converted\")\n",
    "p2, = par1.plot(x, ycount, color=color2, label=\"Click Count\")\n",
    "\n",
    "lns = [p1, p2]\n",
    "host.legend(handles=lns, loc='best')\n",
    "\n",
    "host.yaxis.label.set_color(p1.get_color())\n",
    "par1.yaxis.label.set_color(p2.get_color())\n",
    "\n",
    "plt.savefig(\"pyplot_multiple_y-axis.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only take away I have from this graph is the significant down trend in both Proportion Converted and Click Count that starts around the twelth hour.\n",
    "\n",
    "I am currently unsure what time this would actually represent, nor if it even matters.\n",
    "\n",
    "We'll produce one more graphic to see the `click_hour` vs Converted Ratio, with an error bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('click_hour', 'is_attributed', data=train)\n",
    "plt.title('HOURLY CONVERSION RATIO');\n",
    "plt.ylabel('Converted Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look intro `attributed_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look specifically at conversions that did take place. We'll explicitly be looking at how much time passed from the original ad click to the actual download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['time_pass'] = train['attributed_time']-train['click_time']\n",
    "\n",
    "# peek at the data to ensure its correct\n",
    "train[train['is_attributed']==1][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['time_pass'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the longest time it took to go from clicking an ad to downloading the app was about twenty four hours, while the shortest time was zero seconds.\n",
    "\n",
    "Both this minimum value and maximum value seem incredibly fishy.\n",
    "\n",
    "I was reading through the comments on [yuliagm's post](https://www.kaggle.com/yuliagm/talkingdata-eda-plus-time-patterns) and saw the community discussing what exacty the `attributed_time` and `click_time` might represent. User [shlomis](https://www.kaggle.com/shlomis) suggests that the size of the app would affect the installation time, as the timestamp wouldn't get triggered till the end of the download. So for example, a 100MB app would take longer than say a 10MB app, but [Yuliagm](https://www.kaggle.com/yuliagm) shot that idea down.\n",
    "\n",
    "Additionally, [Peter](https://www.kaggle.com/pestipeti) suggested that the download/install tracking code is in the application itself, atleast for Android, and therefore the `attributed_time` will not get triggered until the user opens the application for the first time. If this was true, this would explain the rather large `time_pass` columns we calculated, as someone could wait almost twenty four hours before opening their app. I find this fishy, as I'd expect the max to be even larger than this. \n",
    "\n",
    "At this point, I nor does anyone I've seen on Kaggle have a definitive idea what these columns represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `click_time` Pandas.DateTime Transformation\n",
    "\n",
    "To make training our model(s) easier, we'll translate our two `datetime64` columns, `click_time` and `attributed_time`, to separate columns that can be represented with `int`s.\n",
    "\n",
    "Specifically:\n",
    "* year\n",
    "* month\n",
    "* day\n",
    "* hour\n",
    "* minute\n",
    "* second\n",
    "\n",
    "We'll be doing this process multicored, as it takes about ten minutes to run on my EC2 `r3.2xlarge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"click_time_year\"], train[\"click_time_month\"], train[\"click_time_day\"], train[\"click_time_hour\"], train[\"click_time_minute\"], train[\"click_time_second\"] = zip(*train[\"click_time\"].apply(lambda row: (row.year, row.month, row.day, row.hour, row.minute, row.second)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# uses all processessors\n",
    "executor = ProcessPoolExecutor(max_workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def convert_date(row):\n",
    "    \"\"\"Converts a single row from Pandas DateTime to individual elements\n",
    "    \"\"\"\n",
    "    \n",
    "    return (row.year, row.month, row.day, row.hour, row.minute, row.second)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results = []\n",
    "\n",
    "# kicking off all calculations\n",
    "for _, row in train.iterrows():\n",
    "    results.append(executor.submit(convert_date, row[\"click_time\"]))\n",
    "\n",
    "# blocking for result\n",
    "results = [i.result() for i in results]\n",
    "\n",
    "train[\"year\"], train[\"month\"], train[\"day\"], train[\"hour\"], train[\"minute\"], train[\"second\"] = zip(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the first couple of rows to ensure we separated the values correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things look good! \n",
    "\n",
    "Now we'll drop the old column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop([\"click_time\"], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Data Leakage\n",
    "\n",
    "To avoid data leakage, our models will not be using the `attributed_time` columns, as this is directly correlated to the `is_attributed` and would result in a score of 100%. Additionaly, the `attributed_time` does not exist in the testing set.\n",
    "\n",
    "Lets remove `attributed_time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>click_time_year</th>\n",
       "      <th>click_time_month</th>\n",
       "      <th>click_time_day</th>\n",
       "      <th>click_time_hour</th>\n",
       "      <th>click_time_minute</th>\n",
       "      <th>click_time_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83230</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:32:21</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17357</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:33:34</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35810</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:34:12</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45745</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>478</td>\n",
       "      <td>2017-11-06 14:34:52</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:35:08</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip app device  os channel          click_time is_attributed  \\\n",
       "0   83230   3      1  13     379 2017-11-06 14:32:21             0   \n",
       "1   17357   3      1  19     379 2017-11-06 14:33:34             0   \n",
       "2   35810   3      1  13     379 2017-11-06 14:34:12             0   \n",
       "3   45745  14      1  13     478 2017-11-06 14:34:52             0   \n",
       "4  161007   3      1  13     379 2017-11-06 14:35:08             0   \n",
       "\n",
       "   click_time_year  click_time_month  click_time_day  click_time_hour  \\\n",
       "0             2017                11               6               14   \n",
       "1             2017                11               6               14   \n",
       "2             2017                11               6               14   \n",
       "3             2017                11               6               14   \n",
       "4             2017                11               6               14   \n",
       "\n",
       "   click_time_minute  click_time_second  \n",
       "0                 32                 21  \n",
       "1                 33                 34  \n",
       "2                 34                 12  \n",
       "3                 34                 52  \n",
       "4                 35                  8  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop([\"attributed_time\"], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we'll be removing `time_pass`, as it was built using `attributed_time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['time_pass'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ec4afa9e6600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time_pass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2530\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   2560\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2563\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   3742\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m                 raise ValueError('labels %s not contained in axis' %\n\u001b[0;32m-> 3744\u001b[0;31m                                  labels[mask])\n\u001b[0m\u001b[1;32m   3745\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['time_pass'] not contained in axis"
     ]
    }
   ],
   "source": [
    "train = train.drop([\"time_pass\"], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to start building models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Our State\n",
    "\n",
    "At this point our data frame is ready to be used my models. Since Jupyter Labs can crash seemingly randomly and working with data this large can throw `MemoryError` exceptions, we're going to pickle the `DataFrame` to ensure we can resume this state without rerunning the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train, open(\"train.p\", \"wb\"), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Our State\n",
    "\n",
    "We can call this method to \"redo\" all the work done above. Calling it after performing save isn't harmful, but is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open(\"train.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Are first model we'll use is strictly logistic regression. Our plan here was to go from nothing to something as quickly as possible and than iterate and see if we can increase our ROC curve value.\n",
    "\n",
    "First we'll get a list of all our features, `x`, and our class variable, `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our x and y column names\n",
    "y_column = [\"is_attributed\"]\n",
    "x_columns = [i for i in train if i not in y_column]\n",
    "\n",
    "x = train[x_columns]\n",
    "y = train[y_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(y_test, y_pred):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "            lw=lw, label='ROC curve (area = %0.2f)' % metrics.auc(fpr, tpr))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Our Data Set Without Stratification\n",
    "\n",
    "So we can ensure we're testing on data we didn't train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Our State\n",
    "\n",
    "Since it takes a while to `train_test_split` our data, we'll save their state incase we need it in the future to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(x_train, open(\"/mnt/secondary_drive/x_train.p\", \"wb\"), protocol=4)\n",
    "pickle.dump(x_test, open(\"/mnt/secondary_drive/x_test.p\", \"wb\"), protocol=4)\n",
    "pickle.dump(y_train, open(\"/mnt/secondary_drive/y_train.p\", \"wb\"), protocol=4)\n",
    "pickle.dump(y_test, open(\"/mnt/secondary_drive/y_test.p\", \"wb\"), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Our State\n",
    "\n",
    "If we need to reload the state, we'll do so without having to redo the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pickle.load(open(\"/mnt/secondary_drive/x_train.p\", \"rb\"))\n",
    "x_test = pickle.load(open(\"/mnt/secondary_drive/x_test.p\", \"rb\"))\n",
    "y_train = pickle.load(open(\"/mnt/secondary_drive/y_train.p\", \"rb\"))\n",
    "y_train = pickle.load(open(\"/mnt/secondary_drive/y_test.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Our Baseline\n",
    "\n",
    "We'll want to be able to compare to guessing `is_attributed` is no for all of them to see how well we perform:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.0 - y_test.sum() / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we should get 0.9974 accuracy by guessing `is_attributed` is $0$ for all values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.roc_auc_score(y_test, [0 for _ in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.accuracy_score(y_test, [0 for _ in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "roc_curve(y_test, [0 for _ in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kaggle competition uses roc auc to score, so our models need to be $0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Logistic Regression\n",
    "\n",
    "Our first attempt will simply use every column as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# we use saga, as the default, liblinear, is good for small data sets\n",
    "# see more here: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "baseline_lr = LogisticRegression(random_state=random_seed, solver=\"saga\")\n",
    "baseline_lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.roc_auc_score(y_test, baseline_lr.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.accuracy_score(y_test, baseline_lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bare minimum work, we're beating our base line, which is great news. Lets take a look at our ROC curve visually:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "roc_curve(y_test, baseline_lr.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Search CV\n",
    "\n",
    "Now we're going to brute force our way through finding the optimal parameters for our Logisitic Regression model, with our goal being that our ROC AUC score is higher than our Base Line Linear Regression."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_search_lr = RandomizedSearchCV(LogisticRegression(), \n",
    "                                   param_distributions={\"tol\": [x * 0.0001 for x in range(1000)],\n",
    "                                                        \"C\": [x * 0.1 for x in range(1, 11)], # this range had to be small, otherwise we'd get a eps <= 0 error\n",
    "                                                        \"fit_intercept\": [True, False],\n",
    "                                                        \"random_state\": [x for x in range(10)] + [random_state]},\n",
    "                                   n_iter=10000, random_state=0, verbose=10, n_jobs=-1, scorer=\"auc\") # TODO change n_iter to 10000 later for better searching\n",
    "\n",
    "# Creating Series out of our y_train DataFrame for fitting\n",
    "y_train_series = y_train[\"is_attributed\"]\n",
    "\n",
    "random_search_lr.fit(x_train, y_train_series)\n",
    "cv_lr = random_search_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.roc_auc_score(y_test, cv_lr.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.accuracy_score(y_test, cv_lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bare minimum work, we're beating our base line, which is great news. Lets take a look at our ROC curve visually:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "roc_curve(y_test, cv_lr.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Our Data With Statisfication\n",
    "\n",
    "Ensure that our training and test sets have equal proportions of our class variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write code to this AND re score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Up Sample Class Variable With SMOTE\n",
    "\n",
    "Ensures we have a data set with closer to 50/50 True/False class variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write code to either up sample or down sample AND rescore, carying over the stratification work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "For fun, lets try using a Random Forest and see how well it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8e03e83f1e58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train the model on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbaseline_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'Timestamp'"
     ]
    }
   ],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "baseline_rf = RandomForestClassifier(random_state=random_seed)\n",
    "\n",
    "# Creating Series out of our y_train DataFrame for fitting\n",
    "y_train_series = y_train[\"is_attributed\"]\n",
    "\n",
    "# Train the model on training data\n",
    "baseline_rf.fit(x_train, y_train_series);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "baseline_rf_predictions = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test, baseline_rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, baseline_rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bare minimum work, we're beating our base line, which is great news. Lets take a look at our ROC curve visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stratify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Search CV\n",
    "\n",
    "Now we're going to brute force our way through finding the optimal parameters for our Logisitic Regression model, with our goal being that our ROC AUC score is higher than our Baseline Random Forest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_search_rf = RandomizedSearchCV(RandomForestClassifier(), \n",
    "                                   param_distributions={\"n_estimators\": [x for x in range(1000)],\n",
    "                                                        \"random_state\": [x for x in range(50)]},\n",
    "                                   n_iter=100, random_state=0, verbose=10, n_jobs=-1, scorer=\"auc\") # TODO change n_iter to 10000 later for better searching\n",
    "\n",
    "# Creating Series out of our y_train DataFrame for fitting\n",
    "y_train_series = y_train[\"is_attributed\"]\n",
    "\n",
    "random_search_rf.fit(x_train, y_train_series)\n",
    "cv_rf = random_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use the forest's predict method on the test data\n",
    "cv_rf_predictions = cv_rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.roc_auc_score(y_test, cv_rf_predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.accuracy_score(y_test, cv_rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bare minimum work, we're beating our base line, which is great news. Lets take a look at our ROC curve visually:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "roc_curve(y_test, cv_rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Aways\n",
    "\n",
    "After going through this entire notebook, I've learned many things that I'll use in a future. Some are:\n",
    "- Use `SFrames` for larger data sets, such as this one.\n",
    "  - This would have greately reduced the amount of hardware needed to perform data manipulation\n",
    "- Perhaps perform EDA on a subset of the data, to ensure the process is **much** quicker, but understanding that the results *may* not represent the population that sample came from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://www.kaggle.com/anokas/talkingdata-adtracking-eda\n",
    "  - for usage of inspecting files before choosing which to use for EDA\n",
    "* https://www.kaggle.com/yuliagm/talkingdata-eda-plus-time-patterns\n",
    "  - for excellent EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
