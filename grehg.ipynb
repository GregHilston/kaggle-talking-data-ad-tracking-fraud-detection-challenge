{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TalkingData AdTracking Fraud Detection Challenge](https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/kernels?sortBy=hotness&group=everyone&pageSize=20&language=Python&competitionId=8540)\n",
    "\n",
    "Can you detect fraudulent click traffic for mobile app ads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "This notebook has crashed every machine I ran it on with less than 60.0 GBs of RAM.\n",
    "\n",
    "_I'm sorry_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure our graphs are displayed inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make wider graphs\n",
    "sns.set(rc={'figure.figsize':(12,5)});\n",
    "plt.figure(figsize=(12,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first look at the data files we've downloaded for the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"File sizes\")\n",
    "\n",
    "for f in os.listdir(\"data/\"):\n",
    "    if \"zip\" not in f and f.endswith(\"csv\"):\n",
    "        print(f.ljust(30) + str(round(os.path.getsize(\"data/\" + f) / 1000000, 2)) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we'd like to work with a small data set, like `train_sample.csv`, its just too small to represent the entire data set `train.csv`. We'll load the entire train set into a DataFrame so we can analyze it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set = \"data/train_sample.csv\"\n",
    "data_set = \"data/train.csv\"\n",
    "\n",
    "train = pd.read_csv(data_set) # , dtype=dtype, parse_dates=parse_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets peak at the first few values of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll notice that the features `ip`, `app`, `device`, `os` and `channel` and our class variable `is_attributed` are categorical as they're encoded to anonymize and preserve privacy. Therefore we'll want to ensure we set their type to non-numerical to avoid nonense operations on the data like calculating their `mean`, `median`, ... etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"ip\", \"app\", \"device\", \"os\", \"channel\", \"is_attributed\"]\n",
    "\n",
    "for column in categorical_columns:\n",
    "    train[column] = train[column].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we'll covert the `click_time` and `attributed_time` columns into date time fields, as they represent time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['click_time'] = pd.to_datetime(train['click_time'])\n",
    "train['attributed_time'] = pd.to_datetime(train['attributed_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get a high level look at the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "cols = [\"ip\", \"app\", \"device\", \"os\", \"channel\"]\n",
    "uniques = [len(train[col].unique()) for col in cols]\n",
    "sns.set(font_scale=1.2)\n",
    "ax = sns.barplot(cols, uniques, log=True)\n",
    "ax.set(xlabel=\"Feature\", ylabel=\"log(unique count)\", title=\"Number of unique values per feature\")\n",
    "\n",
    "# Places the value just above the column\n",
    "for p, uniq in zip(ax.patches, uniques):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 20,\n",
    "            uniq,\n",
    "            ha=\"center\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our knowledge of the competion, every row in the DataFrame that has a set value of `is_attributed` should also have a value for `attributed_time`. Lets test that belief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabs a subset of the DataFrame and then further grabs only the rows where `is_attributed` is set, then calculating the counts\n",
    "train[['attributed_time', 'is_attributed']][train['is_attributed']==1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Take Aways\n",
    "\n",
    "- The training set takes place over two days, two hours and eleven seconds\n",
    "- Out of 184,903,890 rows, only 456,846 of them have an `attributed_time` values of `1.0`\n",
    "  - This means only 456,846 out of 184,903,890 ad clicks resulted in a download\n",
    "  - Which is about 0.0025 % of the clicks\n",
    "- There is atleast one ip adress that triggers an ad click over fifty thousand times\n",
    "  - Seems strange that one ip address would click that often in a span of just 4 days\n",
    "  - Does that mean that ip address encoded is not device id, but network id? (explore this below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is incredilby unbalanced. We're visualizing here the small percents of ad clicks resulting in a download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "#sns.set(font_scale=1.2)\n",
    "mean = (train.is_attributed.values == 1).mean()\n",
    "ax = sns.barplot(['App Downloaded (1)', 'Not Downloaded (0)'], [mean, 1-mean])\n",
    "ax.set(ylabel='Proportion', title='App Downloaded vs Not Downloaded')\n",
    "\n",
    "for p, uniq in zip(ax.patches, [mean, 1-mean]):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height+0.01,\n",
    "            '{}%'.format(round(uniq * 100, 2)),\n",
    "            ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore ip counts: _Check if multuiple ips have any downloads_\n",
    "\n",
    "Since we don't know what `ip` is actually encoding, we're going to see if we can make any inferences based on the `value_counts()` of tha data set.\n",
    "\n",
    "One might think that each `ip` equates to a single user, but we'll see that this is probably not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary table to see ips with their associated count frequencies\n",
    "temp = train['ip'].value_counts().reset_index(name='counts')\n",
    "temp.columns = ['ip', 'counts']\n",
    "temp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add temporary counts of ip feature ('counts') to the train table, to see if IPs with high counts have conversions\n",
    "train= train.merge(temp, on='ip', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check top 10 values\n",
    "train[train['is_attributed']==1].sort_values('counts', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['is_attributed']==1].ip.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see up to `2340` downloads for a single IP address. This IP must be for some sort of network with multiple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'is_attributed' back to numeric for proportion calculations\n",
    "train['is_attributed']=train['is_attributed'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion rates over Counts of 300 most popular IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['ip', 'is_attributed']].groupby('ip', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['ip', 'is_attributed']].groupby('ip', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='ip', how='left')\n",
    "merge.columns = ['ip', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:300].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 300 Most Popular IPs')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular IPs')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be a correlation between the popularity of an `ip` and its `click_count`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by App\n",
    "\n",
    "We'll check out the 100 most popular apps by click count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['app', 'is_attributed']].groupby('app', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['app', 'is_attributed']].groupby('app', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='app', how='left')\n",
    "merge.columns = ['app', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:100].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 100 Most Popular Apps')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular Apps')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here a very large difference in the `click_count` per `app`. The largest `click_count` is thirty three million for one app.\n",
    "\n",
    "We can explain the proportion flucuation as the `click_count` reduces as each click will have a larger impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by OS\n",
    "\n",
    "Now we'll look at the top 100 operating systems by `click_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['os', 'is_attributed']].groupby('os', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['os', 'is_attributed']].groupby('os', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='os', how='left')\n",
    "merge.columns = ['os', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:100].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 100 Most Popular Operating Systems')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular Operating Systems')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agin, we can see ratio is very low but as the `click_count` reduces the ratio starts fluxuating more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['device', 'is_attributed']].groupby('device', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['device', 'is_attributed']].groupby('device', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='device', how='left')\n",
    "merge.columns = ['device', 'click_count', 'prop_downloaded']\n",
    "\n",
    "print('Count of clicks and proportion of downloads by device:')\n",
    "print(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = train[['channel', 'is_attributed']].groupby('channel', as_index=False).mean().sort_values('is_attributed', ascending=False)\n",
    "counts = train[['channel', 'is_attributed']].groupby('channel', as_index=False).count().sort_values('is_attributed', ascending=False)\n",
    "merge = counts.merge(proportion, on='channel', how='left')\n",
    "merge.columns = ['channel', 'click_count', 'prop_downloaded']\n",
    "\n",
    "ax = merge[:100].plot(secondary_y='prop_downloaded')\n",
    "plt.title('Conversion Rates over Counts of 100 Most Popular Apps')\n",
    "ax.set(ylabel='Count of clicks')\n",
    "plt.ylabel('Proportion Downloaded')\n",
    "plt.show()\n",
    "\n",
    "print('Counversion Rates over Counts of Most Popular Channels')\n",
    "print(merge[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some random peaks for some `channel`s at high `click_count`s, but generally we're seeing the same situation as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Time Patterns\n",
    "\n",
    "We're now going to inspect hourly patterns by rounding the `click_time` down to an hour of the samme day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert click_time and attributed_time to time series\n",
    "train['click_time'] = pd.to_datetime(train['click_time'])\n",
    "train['attributed_time'] = pd.to_datetime(train['attributed_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#round the time to nearest hour\n",
    "train['click_rnd']=train['click_time'].dt.round('H')  \n",
    "\n",
    "#check for hourly patterns\n",
    "train[['click_rnd','is_attributed']].groupby(['click_rnd'], as_index=True).count().plot()\n",
    "plt.title('HOURLY CLICK FREQUENCY');\n",
    "plt.ylabel('Number of Clicks');\n",
    "\n",
    "train[['click_rnd','is_attributed']].groupby(['click_rnd'], as_index=True).mean().plot()\n",
    "plt.title('HOURLY CONVERSION RATIO');\n",
    "plt.ylabel('Converted Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's clearly a pattern in frequency of clicks based on time of day, but there is no clear hourly time pattern in ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a new feature by extracting the hour of the day, for every day. We'll be checking if the combined if there's a trend specifically based on the hour throughout the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract hour as a feature\n",
    "train['click_hour']=train['click_time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll inspect the clicks by hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).count().plot(kind='bar', color='#a675a1')\n",
    "plt.title('HOURLY CLICK FREQUENCY Barplot');\n",
    "plt.ylabel('Number of Clicks');\n",
    "\n",
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).count().plot(color='#a675a1')\n",
    "plt.title('HOURLY CLICK FREQUENCY Lineplot');\n",
    "plt.ylabel('Number of Clicks');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, number of conversions by hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).mean().plot(kind='bar', color='#75a1a6')\n",
    "plt.title('HOURLY CONVERSION RATIO Barplot');\n",
    "plt.ylabel('Converted Ratio');\n",
    "\n",
    "train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=True).mean().plot( color='#75a1a6')\n",
    "plt.title('HOURLY CONVERSION RATIO Lineplot');\n",
    "plt.ylabel('Converted Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its hard to compare the clicks and conversions per hour, so lets overlay the two Lineplots to see if there's any easy to see correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from https://stackoverflow.com/questions/9103166/multiple-axis-in-matplotlib-with-different-scales\n",
    "#smonek's answer\n",
    "\n",
    "\n",
    "group = train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=False).mean()\n",
    "x = group['click_hour']\n",
    "ymean = group['is_attributed']\n",
    "group = train[['click_hour','is_attributed']].groupby(['click_hour'], as_index=False).count()\n",
    "ycount = group['is_attributed']\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "host = fig.add_subplot(111)\n",
    "\n",
    "par1 = host.twinx()\n",
    "\n",
    "host.set_xlabel(\"Time\")\n",
    "host.set_ylabel(\"Proportion Converted\")\n",
    "par1.set_ylabel(\"Click Count\")\n",
    "\n",
    "#color1 = plt.cm.viridis(0)\n",
    "#color2 = plt.cm.viridis(0.5)\n",
    "color1 = '#75a1a6'\n",
    "color2 = '#a675a1'\n",
    "\n",
    "p1, = host.plot(x, ymean, color=color1,label=\"Proportion Converted\")\n",
    "p2, = par1.plot(x, ycount, color=color2, label=\"Click Count\")\n",
    "\n",
    "lns = [p1, p2]\n",
    "host.legend(handles=lns, loc='best')\n",
    "\n",
    "host.yaxis.label.set_color(p1.get_color())\n",
    "par1.yaxis.label.set_color(p2.get_color())\n",
    "\n",
    "plt.savefig(\"pyplot_multiple_y-axis.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only take away I have from this graph is the significant down trend in both Proportion Converted and Click Count that starts around the twelth hour.\n",
    "\n",
    "I am currently unsure what time this would actually represent, nor if it even matters.\n",
    "\n",
    "We'll produce one more graphic to see the `click_hour` vs Converted Ratio, with an error bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('click_hour', 'is_attributed', data=train)\n",
    "plt.title('HOURLY CONVERSION RATIO');\n",
    "plt.ylabel('Converted Ratio');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look intro `attributed_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look specifically at conversions that did take place. We'll explicitly be looking at how much time passed from the original ad click to the actual download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['timePass']= train['attributed_time']-train['click_time']\n",
    "#check:\n",
    "train[train['is_attributed']==1][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['timePass'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the longest time it took to go from clicking an ad to downloading the app was about twenty four hours, while the shortest time was zero seconds.\n",
    "\n",
    "Both this minimum value and maximum value seem incredibly fishy.\n",
    "\n",
    "I was reading through the comments on [yuliagm's post](https://www.kaggle.com/yuliagm/talkingdata-eda-plus-time-patterns) and saw the community discussing what exacty the `attributed_time` and `click_time` might represent. User [shlomis](https://www.kaggle.com/shlomis) suggests that the size of the app would affect the installation time, as the timestamp wouldn't get triggered till the end of the download. So for example, a 100MB app would take longer than say a 10MB app, but [Yuliagm](https://www.kaggle.com/yuliagm) shot that idea down.\n",
    "\n",
    "Additionally, [Peter](https://www.kaggle.com/pestipeti) suggested that the download/install tracking code is in the application itself, atleast for Android, and therefore the `attributed_time` will not get triggered until the user opens the application for the first time. If this was true, this would explain the rather large `timePass` columns we calculated, as someone could wait almost twenty four hours before opening their app. I find this fishy, as I'd expect the max to be even larger than this. \n",
    "\n",
    "At this point, I nor does anyone I've seen on Kaggle have a definitive idea what these columns represent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `click_time` Pandas.DateTime Transformation\n",
    "\n",
    "To make training our model(s) easier, we'll translate our two `datetime64` columns, `click_time` and `attributed_time`, to separate columns that can be represented with `int`s.\n",
    "\n",
    "Specifically:\n",
    "* year\n",
    "* month\n",
    "* day\n",
    "* hour\n",
    "* minute\n",
    "* second\n",
    "\n",
    "We'll be doing this process multicored, as it takes about ten minutes to run on my EC2 `r3.2xlarge`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train[\"year\"], train[\"month\"], train[\"day\"], train[\"hour\"], train[\"minute\"], train[\"second\"] = zip(*train[\"click_time\"].apply(lambda row: (row.year, row.month, row.day, row.hour, row.minute, row.second)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses all processessors\n",
    "executor = ProcessPoolExecutor(max_workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(row):\n",
    "    \"\"\"Converts a single row from Pandas DateTime to individual elements\n",
    "    \"\"\"\n",
    "    \n",
    "    return (row.year, row.month, row.day, row.hour, row.minute, row.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# kicking off all calculations\n",
    "for _, row in train.iterrows():\n",
    "    results.append(executor.submit(convert_date, row[\"click_time\"]))\n",
    "\n",
    "# blocking for result\n",
    "results = [i.result() for i in results]\n",
    "\n",
    "train[\"year\"], train[\"month\"], train[\"day\"], train[\"hour\"], train[\"minute\"], train[\"second\"] = zip(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the first couple of rows to ensure we separated the values correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things look good! \n",
    "\n",
    "Now we'll drop the old column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop([\"click_time\"], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Data Leakage\n",
    "\n",
    "To avoid data leakage, our models will not be using the `attributed_time` columns, as this is directly correlated to the `is_attributed` and would result in a score of 100%. Additionaly, the `attributed_time` does not exist in the testing set.\n",
    "\n",
    "Lets remove `attributed_time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop([\"attributed_time\"], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Are first model we'll use is strictly logistic regression. Our plan here was to go from nothing to something as quickly as possible and than iterate and see if we can increase our ROC curve value.\n",
    "\n",
    "First we'll get a list of all our features, `x`, and our class variable, `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our x and y column names\n",
    "y_column = [\"is_attributed\"]\n",
    "x_columns = [i for i in train if i not in y_column]\n",
    "\n",
    "x = train[x_columns]\n",
    "y = train[y_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Metric Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(y_test, y_pred):\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "            lw=lw, label='ROC curve (area = %0.2f)' % metrics.auc(fpr, tpr))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Our Data Set\n",
    "\n",
    "So we can ensure we're testing on data we didn't train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Our Baseline\n",
    "\n",
    "We'll want to be able to compare to guessing `is_attributed` is no for all of them to see how well we perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0 - y_test.sum() / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we should get 0.9974 accuracy by guessing `is_attributed` is $0$ for all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test, [0 for _ in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, [0 for _ in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(y_test, [0 for _ in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Kaggle competition uses roc auc to score, so our models need to be $0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Logistic Regression\n",
    "\n",
    "Our first attempt will simply use every column as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baesline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lr = LogisticRegression()\n",
    "baseline_lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test, baseline_lr.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, baseline_lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bare minimum work, we're beating our base line, which is great news. Lets take a look at our ROC curve visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(y_test, baseline_lr.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Search CV\n",
    "\n",
    "Now we're going to brute force our way through finding the optimal parameters for our Logisitic Regression model, with our goal being that our ROC AUC score is higher than our Base Line Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_lr = RandomizedSearchCV(LogisticRegression(), \n",
    "                                   param_distributions={\"tol\": [x * 0.0001 for x in range(1000)],\n",
    "                                                        \"C\": [x * 0.1 for x in range(1, 10)], # this range had to be small, otherwise we'd get a eps <= 0 error\n",
    "                                                        \"fit_intercept\": [True, False],\n",
    "                                                        \"random_state\": [x for x in range(10)]},\n",
    "                                   n_iter=10000, random_state=0, verbose=1, n_jobs=-1) # TODO change n_iter to 10000 later for better searching\n",
    "\n",
    "# Creating Series out of our y_train DataFrame for fitting\n",
    "y_train_series = y_train[\"is_attributed\"]\n",
    "\n",
    "random_search_lr.fit(x_train, y_train_series)\n",
    "cv_lr = random_search_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test, cv_lr.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, cv_lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bare minimum work, we're beating our base line, which is great news. Lets take a look at our ROC curve visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(y_test, cv_lr.predict_proba(x_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "For fun, lets try using a Random Forest and see how well it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "baseline_rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "\n",
    "# Creating Series out of our y_train DataFrame for fitting\n",
    "y_train_series = y_train[\"is_attributed\"]\n",
    "\n",
    "# Train the model on training data\n",
    "baseline_rf.fit(x_train, y_train_series);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "baseline_rf_predictions = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test, baseline_rf_predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.accuracy_score(y_test, baseline_rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bare minimum work, we're beating our base line, which is great news. Lets take a look at our ROC curve visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Search CV\n",
    "\n",
    "Now we're going to brute force our way through finding the optimal parameters for our Logisitic Regression model, with our goal being that our ROC AUC score is higher than our Baseline Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_rf = RandomizedSearchCV(RandomForestRegressor(), \n",
    "                                   param_distributions={\"n_estimators\": [x for x in range(1000)],\n",
    "                                                        \"random_state\": [x for x in range(50)]},\n",
    "                                   n_iter=100, random_state=0, verbose=1, n_jobs=-1) # TODO change n_iter to 10000 later for better searching\n",
    "\n",
    "# Creating Series out of our y_train DataFrame for fitting\n",
    "y_train_series = y_train[\"is_attributed\"]\n",
    "\n",
    "random_search_rf.fit(x_train, y_train_series)\n",
    "cv_rf = random_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "cv_rf_predictions = cv_rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test, cv_rf_predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "metrics.accuracy_score(y_test, cv_rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bare minimum work, we're beating our base line, which is great news. Lets take a look at our ROC curve visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(y_test, cv_rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://www.kaggle.com/anokas/talkingdata-adtracking-eda\n",
    "  - for usage of inspecting files before choosing which to use for EDA\n",
    "* https://www.kaggle.com/yuliagm/talkingdata-eda-plus-time-patterns\n",
    "  - for excellent EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
